<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>OpenXLA Support on GPU via PJRT &mdash; Intel® Extension for TensorFlow* 0.1.dev1+g8a593c7 documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/sphinx_highlight.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Installation Guide" href="../install/installation_guide.html" />
    <link rel="prev" title="XPUAutoShard on GPU [Experimental]" href="XPUAutoShard.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Intel® Extension for TensorFlow*
          </a>
            <div class="version">
              <a href="../../../versions.html">latest▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../get_started.html">Quick Get Started*</a></li>
<li class="toctree-l1"><a class="reference internal" href="infrastructure.html">Infrastructure</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="features.html">Features</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="features.html#operator-optimization">Operator Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="features.html#graph-optimization">Graph Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="features.html#advanced-auto-mixed-precision-amp">Advanced Auto Mixed Precision (AMP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="features.html#ease-of-use-python-api">Ease-of-use Python API</a></li>
<li class="toctree-l2"><a class="reference internal" href="features.html#gpu-profiler">GPU Profiler</a></li>
<li class="toctree-l2"><a class="reference internal" href="features.html#cpu-launcher-experimental">CPU Launcher [Experimental]</a></li>
<li class="toctree-l2"><a class="reference internal" href="features.html#int8-quantization">INT8 Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="features.html#xpuautoshard-on-gpu-experimental">XPUAutoShard on GPU [Experimental]</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="features.html#openxla-support-on-gpu-experimental">OpenXLA Support on GPU [Experimental]</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">OpenXLA Support on GPU via PJRT</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#overview">1. Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="#hardware-and-software-requirement">2. Hardware and Software Requirement</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#hardware-requirements">Hardware Requirements</a></li>
<li class="toctree-l5"><a class="reference internal" href="#software-requirements">Software Requirements</a></li>
<li class="toctree-l5"><a class="reference internal" href="#install-gpu-drivers">Install GPU Drivers</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#build-library-for-jax">3. Build Library for JAX</a></li>
<li class="toctree-l4"><a class="reference internal" href="#run-jax-example">4. Run JAX Example</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../install/installation_guide.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples/examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="practice_guide.html">Practice Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/releases.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/contributing.html">Contributing guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../SECURITY.html">Security Policy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/intel-extension-for-tensorflow">GitHub Repository</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Intel® Extension for TensorFlow*</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="features.html">Features</a></li>
      <li class="breadcrumb-item active">OpenXLA Support on GPU via PJRT</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/docs/guide/OpenXLA_Support_on_GPU.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="openxla-support-on-gpu-via-pjrt">
<h1>OpenXLA Support on GPU via PJRT<a class="headerlink" href="#openxla-support-on-gpu-via-pjrt" title="Permalink to this heading"></a></h1>
<p>This guide introduces the overview of OpenXLA high level integration structure, and demonstrates how to build Intel® Extension for TensorFlow* and run JAX example with OpenXLA.</p>
<section id="overview">
<h2>1. Overview<a class="headerlink" href="#overview" title="Permalink to this heading"></a></h2>
<p>Intel® Extension for TensorFlow* includes  PJRT plugin implementation, which seamlessly runs JAX models on Intel GPU. The PJRT API simplified the integration, which allowed the Intel GPU plugin to be developed separately and quickly integrated into JAX. This same PJRT implementation also enables initial Intel GPU support for TensorFlow and PyTorch models with XLA acceleration. Refer to <a class="reference external" href="https://github.com/openxla/community/blob/main/rfcs/20230123-pjrt-plugin.md">OpenXLA PJRT Plugin RFC</a> for more details.</p>
<p><img alt="xla" src="../../_images/xla.png" /></p>
<ul>
<li><p><a class="reference external" href="https://jax.readthedocs.io/en/latest/">JAX</a> provides a familiar NumPy-style API, includes composable function transformations for compilation, batching, automatic differentiation, and parallelization, and  the same code executes on multiple backends.</p></li>
<li><p>In JAX python package, <a class="reference external" href="https://github.com/google/jax/blob/jaxlib-v0.4.4/jax/_src/lib/xla_bridge.py#L317-L320"><code class="docutils literal notranslate"><span class="pre">jax/_src/lib/xla_bridge.py</span></code></a></p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span>register_pjrt_plugin_factories(os.getenv(&#39;PJRT_NAMES_AND_LIBRARY_PATHS&#39;, &#39;&#39;))
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">register_pjrt_plugin_factories</span></code> registers backend for PJRT plugins. For intel XPU  <code class="docutils literal notranslate"><span class="pre">PJRT_NAMES_AND_LIBRARY_PATHS</span></code> is set to be <code class="docutils literal notranslate"><span class="pre">'xpu:Your_itex_path/bazel-bin/itex/libitex_xla_extension.so'</span></code>,  <code class="docutils literal notranslate"><span class="pre">xpu</span></code> is the backend name and <code class="docutils literal notranslate"><span class="pre">libitex_xla_extension.so</span></code> is the PJRT plugin library.</p>
</li>
<li><p>In jaxlib python package <code class="docutils literal notranslate"><span class="pre">jaxlib/xla_extension.so</span></code>,<br />Jaxlib gets the lastest tensorflow code which calls the <a class="reference external" href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/pjrt/c/pjrt_c_api.h">PJRT C API interface</a>. The backend needs to implement these API.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">libitex_xla_extension.so</span></code> implements <code class="docutils literal notranslate"><span class="pre">PJRT</span> <span class="pre">C</span> <span class="pre">API</span> <span class="pre">interface</span></code> which can be got in <a class="reference external" href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/pjrt/pjrt_api.cc#L82">GetPjrtApi</a>.</p></li>
</ul>
</section>
<section id="hardware-and-software-requirement">
<h2>2. Hardware and Software Requirement<a class="headerlink" href="#hardware-and-software-requirement" title="Permalink to this heading"></a></h2>
<section id="hardware-requirements">
<h3>Hardware Requirements<a class="headerlink" href="#hardware-requirements" title="Permalink to this heading"></a></h3>
<p>Verified Hardware Platforms:</p>
<ul class="simple">
<li><p>Intel® Data Center GPU Max Series, Driver Version: <a class="reference external" href="https://dgpu-docs.intel.com/releases/stable_647_21_20230714.htmll">647</a></p></li>
<li><p>Intel® Data Center GPU Flex Series 170, Driver Version: <a class="reference external" href="https://dgpu-docs.intel.com/releases/stable_647_21_20230714.html">647</a></p></li>
<li><p><em>Experimental:</em> Intel® Arc™ A-Series</p></li>
</ul>
</section>
<section id="software-requirements">
<h3>Software Requirements<a class="headerlink" href="#software-requirements" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>Ubuntu 22.04, Red Hat 8.6 (64-bit)</p>
<ul>
<li><p>Intel® Data Center GPU Flex Series</p></li>
</ul>
</li>
<li><p>Ubuntu 22.04, Red Hat 8.6 (64-bit), SUSE Linux Enterprise Server(SLES) 15 SP3/SP4</p>
<ul>
<li><p>Intel® Data Center GPU Max Series</p></li>
</ul>
</li>
<li><p>Intel® oneAPI Base Toolkit 2023.2</p></li>
<li><p>TensorFlow 2.13.0</p></li>
<li><p>Python 3.8-3.10</p></li>
<li><p>pip 19.0 or later (requires manylinux2014 support)</p></li>
</ul>
</section>
<section id="install-gpu-drivers">
<h3>Install GPU Drivers<a class="headerlink" href="#install-gpu-drivers" title="Permalink to this heading"></a></h3>
<table border="1" class="docutils">
<thead>
<tr>
<th>OS</th>
<th>Intel GPU</th>
<th>Install Intel GPU Driver</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ubuntu 22.04, Red Hat 8.6</td>
<td>Intel® Data Center GPU Flex Series</td>
<td>Refer to the <a href="https://dgpu-docs.intel.com/installation-guides/index.html#intel-data-center-gpu-flex-series">Installation Guides</a> for latest driver installation. If install the verified Intel® Data Center GPU Max Series/Intel® Data Center GPU Flex Series <a href="https://dgpu-docs.intel.com/releases/stable_647_21_20230714.html">647</a>, please append the specific version after components, such as <code>sudo apt-get install intel-opencl-icd==23.17.26241.33-647~22.04</code></td>
</tr>
<tr>
<td>Ubuntu 22.04, Red Hat 8.6, SLES 15 SP3/SP4</td>
<td>Intel® Data Center GPU Max Series</td>
<td>Refer to the <a href="https://dgpu-docs.intel.com/installation-guides/index.html#intel-data-center-gpu-max-series">Installation Guides</a> for latest driver installation. If install the verified Intel® Data Center GPU Max Series/Intel® Data Center GPU Flex Series <a href="https://dgpu-docs.intel.com/releases/stable_647_21_20230714.html">647</a>, please append the specific version after components, such as <code>sudo apt-get install intel-opencl-icd==23.17.26241.33-647~22.04</code></td>
</tr>
</tbody>
</table></section>
</section>
<section id="build-library-for-jax">
<h2>3. Build Library for JAX<a class="headerlink" href="#build-library-for-jax" title="Permalink to this heading"></a></h2>
<p>There are some differences from   <a class="reference external" href="https://github.com/intel/intel-extension-for-tensorflow/blob/main/docs/install/how_to_build.md">source build procedure</a></p>
<ul>
<li><p>Make sure get Intel® Extension for TensorFlow* main branch code and python version &gt;=3.8.</p></li>
<li><p>In TensorFlow installation steps, make sure to install jax and jaxlib at the same time.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="w"> </span>$<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span><span class="nv">tensorflow</span><span class="o">==</span><span class="m">2</span>.13.0<span class="w"> </span><span class="nv">jax</span><span class="o">==</span><span class="m">0</span>.4.4<span class="w"> </span><span class="nv">jaxlib</span><span class="o">==</span><span class="m">0</span>.4.4
</pre></div>
</div>
</li>
<li><p>In “Configure the build” step, run ./configure, select yes for JAX support,</p>
<blockquote>
<div><p>=&gt; “Do you wish to build for JAX support? [y/N]: Y”</p>
</div></blockquote>
</li>
<li><p>Build command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>bazel<span class="w"> </span>build<span class="w"> </span>--config<span class="o">=</span>jax<span class="w"> </span>-c<span class="w"> </span>opt<span class="w"> </span>//itex:libitex_xla_extension.so
</pre></div>
</div>
</li>
</ul>
<p>Then we can get the library with xla extension   <strong>./bazel-bin/itex/libitex_xla_extension.so</strong></p>
</section>
<section id="run-jax-example">
<h2>4. Run JAX Example<a class="headerlink" href="#run-jax-example" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><strong>Set library path.</strong></p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">PJRT_NAMES_AND_LIBRARY_PATHS</span><span class="o">=</span><span class="s1">&#39;xpu:Your_itex_path/bazel-bin/itex/libitex_xla_extension.so&#39;</span>
$<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:Your_Python_site-packages/jaxlib<span class="w"> </span><span class="c1"># Some functions defined in xla_extension.so are needed by libitex_xla_extension.so</span>

$<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">ITEX_VERBOSE</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="c1"># Optional variable setting. It shows detailed optimization/compilation/execution info.</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>Run the below jax python code.</strong></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>

<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">lax_conv</span><span class="p">():</span>
  <span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">lhs</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">9</span><span class="p">),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">rhs</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">side</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">out</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">conv_with_general_padding</span><span class="p">(</span><span class="n">lhs</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">)),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
  <span class="n">out</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
  <span class="n">out</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">side</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">out</span>

<span class="nb">print</span><span class="p">(</span><span class="n">lax_conv</span><span class="p">())</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>Reference result:</strong></p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">I</span> <span class="n">itex</span><span class="o">/</span><span class="n">core</span><span class="o">/</span><span class="n">devices</span><span class="o">/</span><span class="n">gpu</span><span class="o">/</span><span class="n">itex_gpu_runtime</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">129</span><span class="p">]</span> <span class="n">Selected</span> <span class="n">platform</span><span class="p">:</span> <span class="n">Intel</span><span class="p">(</span><span class="n">R</span><span class="p">)</span> <span class="n">Level</span><span class="o">-</span><span class="n">Zero</span>
<span class="n">I</span> <span class="n">itex</span><span class="o">/</span><span class="n">core</span><span class="o">/</span><span class="n">compiler</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">service</span><span class="o">/</span><span class="n">service</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">176</span><span class="p">]</span> <span class="n">XLA</span> <span class="n">service</span> <span class="mh">0x56060b5ae740</span> <span class="n">initialized</span> <span class="k">for</span> <span class="n">platform</span> <span class="n">sycl</span> <span class="p">(</span><span class="n">this</span> <span class="n">does</span> <span class="ow">not</span> <span class="n">guarantee</span> <span class="n">that</span> <span class="n">XLA</span> <span class="n">will</span> <span class="n">be</span> <span class="n">used</span><span class="p">)</span><span class="o">.</span> <span class="n">Devices</span><span class="p">:</span>
<span class="n">I</span> <span class="n">itex</span><span class="o">/</span><span class="n">core</span><span class="o">/</span><span class="n">compiler</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">service</span><span class="o">/</span><span class="n">service</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">184</span><span class="p">]</span>   <span class="n">StreamExecutor</span> <span class="n">device</span> <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="o">&lt;</span><span class="n">undefined</span><span class="o">&gt;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">undefined</span><span class="o">&gt;</span>
<span class="n">I</span> <span class="n">itex</span><span class="o">/</span><span class="n">core</span><span class="o">/</span><span class="n">compiler</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">service</span><span class="o">/</span><span class="n">service</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">184</span><span class="p">]</span>   <span class="n">StreamExecutor</span> <span class="n">device</span> <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="o">&lt;</span><span class="n">undefined</span><span class="o">&gt;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">undefined</span><span class="o">&gt;</span>
<span class="p">[[[[</span><span class="mf">2.0449753</span> <span class="mf">2.093208</span>  <span class="mf">2.1844783</span> <span class="mf">1.9769732</span> <span class="mf">1.5857391</span> <span class="mf">1.6942389</span><span class="p">]</span>
   <span class="p">[</span><span class="mf">1.9218378</span> <span class="mf">2.2862523</span> <span class="mf">2.1549542</span> <span class="mf">1.8367321</span> <span class="mf">1.3978379</span> <span class="mf">1.3860377</span><span class="p">]</span>
   <span class="p">[</span><span class="mf">1.9456574</span> <span class="mf">2.062028</span>  <span class="mf">2.0365305</span> <span class="mf">1.901286</span>  <span class="mf">1.5255247</span> <span class="mf">1.1421617</span><span class="p">]</span>
   <span class="p">[</span><span class="mf">2.0621</span>    <span class="mf">2.2933435</span> <span class="mf">2.1257985</span> <span class="mf">2.1095486</span> <span class="mf">1.5584903</span> <span class="mf">1.1229166</span><span class="p">]</span>
   <span class="p">[</span><span class="mf">1.7746235</span> <span class="mf">2.2446113</span> <span class="mf">1.7870374</span> <span class="mf">1.8216239</span> <span class="mf">1.557919</span>  <span class="mf">0.9832508</span><span class="p">]</span>
   <span class="p">[</span><span class="mf">2.0887792</span> <span class="mf">2.5433128</span> <span class="mf">1.9749291</span> <span class="mf">2.2580051</span> <span class="mf">1.6096935</span> <span class="mf">1.264905</span> <span class="p">]]]</span>


 <span class="p">[[[</span><span class="mf">2.175818</span>  <span class="mf">2.0094342</span> <span class="mf">2.005763</span>  <span class="mf">1.6559253</span> <span class="mf">1.3896458</span> <span class="mf">1.4036925</span><span class="p">]</span>
   <span class="p">[</span><span class="mf">2.1342552</span> <span class="mf">1.8239582</span> <span class="mf">1.6091168</span> <span class="mf">1.434404</span>  <span class="mf">1.671778</span>  <span class="mf">1.7397764</span><span class="p">]</span>
   <span class="p">[</span><span class="mf">1.930626</span>  <span class="mf">1.659667</span>  <span class="mf">1.6508744</span> <span class="mf">1.3305787</span> <span class="mf">1.4061482</span> <span class="mf">2.0829628</span><span class="p">]</span>
   <span class="p">[</span><span class="mf">2.130649</span>  <span class="mf">1.6637266</span> <span class="mf">1.594426</span>  <span class="mf">1.2636002</span> <span class="mf">1.7168686</span> <span class="mf">1.8598001</span><span class="p">]</span>
   <span class="p">[</span><span class="mf">1.9009514</span> <span class="mf">1.7938274</span> <span class="mf">1.4870623</span> <span class="mf">1.6193901</span> <span class="mf">1.5297288</span> <span class="mf">2.0247464</span><span class="p">]</span>
   <span class="p">[</span><span class="mf">2.0905268</span> <span class="mf">1.7598859</span> <span class="mf">1.9362347</span> <span class="mf">1.9513799</span> <span class="mf">1.9403584</span> <span class="mf">2.1483061</span><span class="p">]]]]</span>
</pre></div>
</div>
<p>If <code class="docutils literal notranslate"><span class="pre">ITEX_VERBOSE=1</span></code> is set, the log looks like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">I</span> <span class="n">itex</span><span class="o">/</span><span class="n">core</span><span class="o">/</span><span class="n">compiler</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">service</span><span class="o">/</span><span class="n">hlo_pass_pipeline</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">301</span><span class="p">]</span> <span class="n">Running</span> <span class="n">HLO</span> <span class="k">pass</span> <span class="n">pipeline</span> <span class="n">on</span> <span class="n">module</span> <span class="n">jit_lax_conv</span><span class="p">:</span> <span class="n">optimization</span>
<span class="n">I</span> <span class="n">itex</span><span class="o">/</span><span class="n">core</span><span class="o">/</span><span class="n">compiler</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">service</span><span class="o">/</span><span class="n">hlo_pass_pipeline</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">181</span><span class="p">]</span>   <span class="n">HLO</span> <span class="k">pass</span> <span class="n">fusion</span>
<span class="n">I</span> <span class="n">itex</span><span class="o">/</span><span class="n">core</span><span class="o">/</span><span class="n">compiler</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">service</span><span class="o">/</span><span class="n">hlo_pass_pipeline</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">181</span><span class="p">]</span>   <span class="n">HLO</span> <span class="k">pass</span> <span class="n">fusion_merger</span>
<span class="n">I</span> <span class="n">itex</span><span class="o">/</span><span class="n">core</span><span class="o">/</span><span class="n">compiler</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">service</span><span class="o">/</span><span class="n">hlo_pass_pipeline</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">181</span><span class="p">]</span>   <span class="n">HLO</span> <span class="k">pass</span> <span class="n">multi_output_fusion</span>
<span class="n">I</span> <span class="n">itex</span><span class="o">/</span><span class="n">core</span><span class="o">/</span><span class="n">compiler</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">service</span><span class="o">/</span><span class="n">hlo_pass_pipeline</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">181</span><span class="p">]</span>   <span class="n">HLO</span> <span class="k">pass</span> <span class="n">gpu</span><span class="o">-</span><span class="n">conv</span><span class="o">-</span><span class="n">rewriter</span>
<span class="n">I</span> <span class="n">itex</span><span class="o">/</span><span class="n">core</span><span class="o">/</span><span class="n">compiler</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">service</span><span class="o">/</span><span class="n">hlo_pass_pipeline</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">181</span><span class="p">]</span>   <span class="n">HLO</span> <span class="k">pass</span> <span class="n">onednn</span><span class="o">-</span><span class="n">fused</span><span class="o">-</span><span class="n">convolution</span><span class="o">-</span><span class="n">rewriter</span>

<span class="n">I</span> <span class="n">itex</span><span class="o">/</span><span class="n">core</span><span class="o">/</span><span class="n">compiler</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">service</span><span class="o">/</span><span class="n">gpu</span><span class="o">/</span><span class="n">gpu_compiler</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">1221</span><span class="p">]</span> <span class="n">Build</span> <span class="n">kernel</span> <span class="n">via</span> <span class="n">LLVM</span> <span class="n">kernel</span> <span class="n">compilation</span><span class="o">.</span>
<span class="n">I</span> <span class="n">itex</span><span class="o">/</span><span class="n">core</span><span class="o">/</span><span class="n">compiler</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">service</span><span class="o">/</span><span class="n">gpu</span><span class="o">/</span><span class="n">spir_compiler</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">255</span><span class="p">]</span> <span class="n">CompileTargetBinary</span> <span class="o">-</span> <span class="n">CompileToSpir</span> <span class="n">time</span><span class="p">:</span> <span class="mi">11</span> <span class="n">us</span> <span class="p">(</span><span class="n">cumulative</span><span class="p">:</span> <span class="mf">99.2</span> <span class="n">ms</span><span class="p">,</span> <span class="nb">max</span><span class="p">:</span> <span class="mf">74.9</span> <span class="n">ms</span><span class="p">,</span> <span class="c1">#called: 8)</span>

<span class="n">I</span> <span class="n">itex</span><span class="o">/</span><span class="n">core</span><span class="o">/</span><span class="n">compiler</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">pjrt</span><span class="o">/</span><span class="n">pjrt_stream_executor_client</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">2201</span><span class="p">]</span> <span class="n">Executing</span> <span class="n">computation</span> <span class="n">jit_lax_conv</span><span class="p">;</span> <span class="n">num_replicas</span><span class="o">=</span><span class="mi">1</span> <span class="n">num_partitions</span><span class="o">=</span><span class="mi">1</span> <span class="n">num_addressable_devices</span><span class="o">=</span><span class="mi">1</span>
<span class="n">I</span> <span class="n">itex</span><span class="o">/</span><span class="n">core</span><span class="o">/</span><span class="n">compiler</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">pjrt</span><span class="o">/</span><span class="n">pjrt_stream_executor_client</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">2268</span><span class="p">]</span> <span class="n">Replicated</span> <span class="n">execution</span> <span class="n">complete</span><span class="o">.</span>
<span class="n">I</span> <span class="n">itex</span><span class="o">/</span><span class="n">core</span><span class="o">/</span><span class="n">compiler</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">pjrt</span><span class="o">/</span><span class="n">pjrt_stream_executor_client</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">1208</span><span class="p">]</span> <span class="n">PjRtStreamExecutorBuffer</span><span class="p">::</span><span class="n">Delete</span>
<span class="n">I</span> <span class="n">itex</span><span class="o">/</span><span class="n">core</span><span class="o">/</span><span class="n">compiler</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">pjrt</span><span class="o">/</span><span class="n">pjrt_stream_executor_client</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">1299</span><span class="p">]</span> <span class="n">PjRtStreamExecutorBuffer</span><span class="p">::</span><span class="n">ToLiteral</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>More JAX examples</strong><br />Get examples from <a class="reference external" href="https://github.com/google/jax/tree/jaxlib-v0.4.4/examples">https://github.com/google/jax</a> to run.</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/google/jax.git
$<span class="w"> </span><span class="nb">cd</span><span class="w"> </span>jax<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>git<span class="w"> </span>checkout<span class="w"> </span>jax-v0.4.4
$<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">PJRT_NAMES_AND_LIBRARY_PATHS</span><span class="o">=</span><span class="s1">&#39;xpu:Your_itex_path/bazel-bin/itex/libitex_xla_extension.so&#39;</span>
$<span class="w"> </span>python<span class="w"> </span>-m<span class="w"> </span>examples.mnist_classifier
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="XPUAutoShard.html" class="btn btn-neutral float-left" title="XPUAutoShard on GPU [Experimental]" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../install/installation_guide.html" class="btn btn-neutral float-right" title="Installation Guide" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022-2023 Intel Corporation.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>